{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCN8 VOC.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZUqRCGRqynCRDzDNsVvSU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "defbc851edbb4943b9e0acf81d91d3d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c5df5549e8c04004aa8ccbff7014435d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89eb8257a288498ba86c264b3d233568",
              "IPY_MODEL_832069ec93e243428b1f0f2f81665ee3"
            ]
          }
        },
        "c5df5549e8c04004aa8ccbff7014435d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89eb8257a288498ba86c264b3d233568": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_04bfc0d1421c497c90772304d5305da7",
            "_dom_classes": [],
            "description": "Loading Images...:   1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2223,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 17,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a928686342a43299196c4f8f8ca2249"
          }
        },
        "832069ec93e243428b1f0f2f81665ee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_211aa7aa69e84fbf8c3dd78b749d8b32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 17/2223 [00:00&lt;01:00, 36.50it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb851183452a49c0b9b203c4cb8e8166"
          }
        },
        "04bfc0d1421c497c90772304d5305da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a928686342a43299196c4f8f8ca2249": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "211aa7aa69e84fbf8c3dd78b749d8b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb851183452a49c0b9b203c4cb8e8166": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giacomogreggio/computer-vision-project/blob/master/FCN8_VOC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmZzxix0jlGY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "24c76d7f-234d-497f-9dae-6a9272cd6c6e"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHdpH7h8jxlG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if \"JPEGImagesMax\" not in os.listdir(\"/content\"):\n",
        "    !unzip \"/content/drive/My Drive/Colab Notebooks/dati/JPEGImagesMax.zip\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qone-xrUkCHM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if \"SegmentationClassConvertedMin\" not in os.listdir(\"/content\"):\n",
        "    !unzip \"/content/drive/My Drive/Colab Notebooks/dati/SegmentationClassConvertedMin.zip\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY6hIbAPg0FZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as back \n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input \n",
        "import os\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, MaxPool2D, ZeroPadding2D, Cropping2D, Softmax, Add, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.metrics import MeanIoU\n",
        "%matplotlib inline\n",
        "import h5py\n",
        "from IPython.display import display, clear_output\n",
        "from PIL import Image\n",
        "from skimage.io import imshow\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, CategoricalCrossentropy\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from tqdm.notebook import trange, tqdm \n",
        "from time import sleep  \n",
        "import random"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeuxgsZ2g2jk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_color_map(N=256, normalized=False):\n",
        "    def bitget(byteval, idx):\n",
        "        return ((byteval & (1 << idx)) != 0)\n",
        "\n",
        "    dtype = 'float32' if normalized else 'uint8'\n",
        "    cmap = np.zeros((N, 3), dtype=dtype)\n",
        "    for i in range(N):\n",
        "        r = g = b = 0\n",
        "        c = i\n",
        "        for j in range(8):\n",
        "            r = r | (bitget(c, 0) << 7-j)\n",
        "            g = g | (bitget(c, 1) << 7-j)\n",
        "            b = b | (bitget(c, 2) << 7-j)\n",
        "            c = c >> 3\n",
        "\n",
        "        cmap[i] = np.array([r, g, b])\n",
        "\n",
        "    cmap = cmap/255 if normalized else cmap\n",
        "    return cmap[0:N]\n",
        "\n",
        "\n",
        "def color_map_viz():\n",
        "    labels = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor', 'void']\n",
        "    nclasses = 21\n",
        "    row_size = 50\n",
        "    col_size = 500\n",
        "    cmap = get_color_map()\n",
        "    array = np.empty((row_size*(nclasses+1), col_size, cmap.shape[1]), dtype=cmap.dtype)\n",
        "    for i in range(nclasses):\n",
        "        array[i*row_size:i*row_size+row_size, :] = cmap[i]\n",
        "    array[nclasses*row_size:nclasses*row_size+row_size, :] = cmap[-1]\n",
        "    imshow(array)\n",
        "    plt.yticks([row_size*i+row_size/2 for i in range(nclasses+1)], labels)\n",
        "    plt.xticks([])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_color_dict(n_classes):\n",
        "    cmap = get_color_map(N=n_classes).tolist()\n",
        "    colored_map = cmap[0:n_classes]\n",
        "    color_dict = {}\n",
        "    for i in range(len(colored_map)):\n",
        "        color_dict[tuple(colored_map[i])] = i\n",
        "    return color_dict"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDOVh7EWg--b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_img(img):\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.subplot(1,1,1), plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB)), plt.title('Lena'), plt.axis('off')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_J275MFhBC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nclasses = 22\n",
        "colored_map = get_color_map(N=nclasses)\n",
        "color_dict = get_color_dict(nclasses)\n",
        "dataset_path = \"/content\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqx39sauhnkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(input_image):\n",
        "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "  return input_image"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkwsjutHhE8z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "defbc851edbb4943b9e0acf81d91d3d0",
            "c5df5549e8c04004aa8ccbff7014435d",
            "89eb8257a288498ba86c264b3d233568",
            "832069ec93e243428b1f0f2f81665ee3",
            "04bfc0d1421c497c90772304d5305da7",
            "0a928686342a43299196c4f8f8ca2249",
            "211aa7aa69e84fbf8c3dd78b749d8b32",
            "eb851183452a49c0b9b203c4cb8e8166"
          ]
        },
        "outputId": "b572253f-6969-4671-a6ae-e232bb7ae47e"
      },
      "source": [
        "dataset = []\n",
        "targets = []\n",
        "images = os.listdir(dataset_path + \"/SegmentationClassConvertedMin\")\n",
        "for i in tqdm(range(len(images)), desc='Loading Images...'):   \n",
        "    \n",
        "    name = images[i].split(\".\")[0]\n",
        "    img_x = cv2.imread(dataset_path + '/JPEGImagesMax/' + name + '.jpg',1)\n",
        "    dataset.append(normalize(img_x).numpy())\n",
        "    \n",
        "    with open(dataset_path + '/SegmentationClassConvertedMin/' + name + \".npy\", 'rb') as f:\n",
        "        targets.append(np.load(f))\n",
        "    if i==20:\n",
        "        break\n",
        "  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "defbc851edbb4943b9e0acf81d91d3d0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Loading Images...', max=2223.0, style=ProgressStyle(descr…"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW01Joeb0C4B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_3_rU6khTBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 1000\n",
        "EPOCHS = 100\n",
        "VAL_SUBSPLITS = 5\n",
        "\n",
        "TRAIN_LENGTH = len('/content/SegmentationClassConvertedMin')\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hsKmdMhiV9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "  display_list[1] = np.expand_dims(display_list[1],axis=2)\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask[0]\n",
        "\n",
        "\n",
        "def show_predictions(num=1):\n",
        "  for i in range(num if num < len(dataset) else len(dataset)):  \n",
        "    n = len(dataset)-i-1\n",
        "    pred_mask = create_mask(model.predict(np.expand_dims(dataset[i],axis=0)))\n",
        "    display([dataset[n], targets[n], pred_mask])\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN2B_uIbie5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_shape=(500,500,3)\n",
        "n_classes=22\n",
        "l2_value=5**-4\n",
        "crop_value=((16,16),(16,16))\n",
        "def create_base_vgg(trainable=True):\n",
        "  #Defining Base VGG architecture\n",
        "  input_layer = Input(shape=image_shape, name=\"input\")\n",
        "  #VGG-block1\n",
        "  b1 = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name=\"conv2d_b1_1\")(input_layer)\n",
        "  b1 = Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\", name=\"conv2d_b1_2\")(b1)\n",
        "  b1 = MaxPool2D(pool_size=(2,2),strides=(2,2), name=\"maxpool_b1\")(b1)\n",
        "\n",
        "  #VGG-block2\n",
        "  b2 = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b2_1\")(b1)\n",
        "  b2 = Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b2_2\")(b2)\n",
        "  b2 = MaxPool2D(pool_size=(2,2),strides=(2,2), name=\"maxpool_b2\")(b2)\n",
        "\n",
        "  #VGG-block3\n",
        "  b3 = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b3_1\")(b2)\n",
        "  b3 = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b3_2\")(b3)\n",
        "  b3 = Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b3_3\")(b3)\n",
        "  b3 = MaxPool2D(pool_size=(2,2),strides=(2,2), name=\"maxpool_b3\")(b3)\n",
        "\n",
        "  #VGG-block4\n",
        "  b4 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b4_1\")(b3)\n",
        "  b4 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b4_2\")(b4)\n",
        "  b4 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b4_3\")(b4)\n",
        "  b4 = MaxPool2D(pool_size=(2,2),strides=(2,2), name=\"maxpool_b4\")(b4)\n",
        "\n",
        "  #VGG-block5\n",
        "  b5 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b5_1\")(b4)\n",
        "  b5 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b5_2\")(b5)\n",
        "  b5 = Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\", name=\"conv2d_b5_3\")(b5)\n",
        "  b5 = MaxPool2D(pool_size=(2,2),strides=(2,2), name=\"maxpool_b5\")(b5)\n",
        "\n",
        "  vgg_model = Model(input_layer, b5)\n",
        "  vgg16= VGG16(weights=\"imagenet\", include_top=False)\n",
        "  vgg16.save_weights(\"./weights.h5\")\n",
        "  vgg_model.load_weights(\"./weights.h5\")\n",
        "  vgg_model.trainable=trainable\n",
        "\n",
        "  return vgg_model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k59Qj-78ih6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fcn32(vgg_trainable=True, include_top=True):\n",
        "  vgg_model=create_base_vgg(trainable=vgg_trainable)\n",
        "  b5=vgg_model.layers[-1].output\n",
        "  input_layer=vgg_model.layers[0].output\n",
        "  fcn_32_block = Conv2D(4096, kernel_size=(7,7), activation='relu', padding=\"same\", kernel_regularizer=l2(l2_value), name=\"conv2d_fcn32_1\")(b5)\n",
        "  fcn_32_block = Dropout(0.5, name=\"dropout_fcn32_1\")(fcn_32_block)\n",
        "  fcn_32_block = Conv2D(4096, kernel_size=(1,1), activation='relu', padding=\"same\", kernel_regularizer=l2(l2_value), name=\"conv2d_fcn32_2\")(fcn_32_block)\n",
        "  fcn_32_block = Dropout(0.5, name=\"dropout_fcn32_2\")(fcn_32_block)\n",
        "  fcn_32_block = Conv2D(n_classes, kernel_size=(1,1), padding=\"same\", kernel_regularizer=l2(l2_value), name=\"conv2d_fcn32_3\")(fcn_32_block)\n",
        "  if include_top:\n",
        "    fcn_32_transpose = Conv2DTranspose(n_classes, kernel_size=(64,64), strides=(32,32), kernel_regularizer=l2(l2_value), name=\"deconv_fcn32\")(fcn_32_block)\n",
        "    fcn_32_crop = Cropping2D(crop_value, name=\"crop_fcn32\")(fcn_32_transpose)\n",
        "    fcn_32_softmax=Softmax(name=\"softmax_fcn32\")(fcn_32_crop)\n",
        "    return Model(input_layer, fcn_32_softmax)\n",
        "  else:\n",
        "    return Model(input_layer, fcn_32_block)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpUC2eA8ijQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fcn16(vgg_trainable=True, include_top=True):\n",
        "  fcn_32=create_fcn32(vgg_trainable=vgg_trainable, include_top=False)\n",
        "  input_layer=fcn_32.layers[0].output\n",
        "  b4=fcn_32.get_layer(\"maxpool_b4\").output\n",
        "  fcn_32_block=fcn_32.layers[-1].output\n",
        "\n",
        "  fcn_16_block_fcn32 = Conv2DTranspose(n_classes, kernel_size=(4,4), padding=\"valid\", strides=(2,2), kernel_regularizer=l2(l2_value), name=\"deconv_fcn16_1\")(fcn_32_block)\n",
        "  fcn_16_block_fcn32 = Cropping2D(((1,0,),(1,0)), name=\"crop_fnc16_1\")(fcn_16_block_fcn32)\n",
        "\n",
        "  fcn_16_block = Conv2D(n_classes, kernel_size=(1,1), activation=\"relu\", kernel_regularizer=l2(l2_value), padding=\"valid\", name=\"conv2d_fcn16_1\")(b4)\n",
        "  fcn_16_block = Add(name=\"add_fcn16\")([fcn_16_block_fcn32,fcn_16_block])\n",
        "\n",
        "  if include_top:\n",
        "    fcn_16_deconv = Conv2DTranspose(n_classes, kernel_size=(32,32), strides=(16,16), kernel_regularizer=l2(l2_value), name=\"deconv_fcn16_2\")(fcn_16_block)\n",
        "    fcn_16_crop = Cropping2D(((8,8,),(8,8)), name=\"crop_fcn16_2\")(fcn_16_deconv)\n",
        "    fcn_16_softmax=Softmax(name=\"softmax_fcn16\")(fcn_16_crop)\n",
        "    return Model(input_layer, fcn_16_softmax)\n",
        "  else:\n",
        "    return Model(input_layer, fcn_16_block)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6GLdzw6iklF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_fcn8(vgg_trainable=True, include_top=True):\n",
        "  fcn_16=create_fcn16(vgg_trainable=vgg_trainable, include_top=False)\n",
        "  input_layer=fcn_16.layers[0].output\n",
        "  b3=fcn_16.get_layer(\"maxpool_b3\").output\n",
        "  fcn_16_block=fcn_16.layers[-1].output\n",
        "\n",
        "  fcn_8_block_fcn16 = Conv2DTranspose(n_classes, kernel_size=(4,4), padding=\"valid\", strides=(2,2), kernel_regularizer=l2(l2_value), name=\"deconv_fnc8_1\")(fcn_16_block)\n",
        "  fcn_8_block_fcn16 = Cropping2D(((1,1,),(1,1)), name=\"crop_fnc8_1\")(fcn_8_block_fcn16)\n",
        "\n",
        "  fcn_8_block = Conv2D(n_classes, kernel_size=(1,1), activation=\"relu\", kernel_regularizer=l2(l2_value), padding=\"valid\", name=\"conv2d_fcn8_1\")(b3)\n",
        "  fcn_8_block = Add(name=\"add_fcn8\")([fcn_8_block_fcn16,fcn_8_block])\n",
        "\n",
        "  if include_top:\n",
        "    fcn_8_deconv = Conv2DTranspose(n_classes, kernel_size=(16,16), strides=(8,8), kernel_regularizer=l2(l2_value), name=\"deconv_fnc8_2\")(fcn_8_block)\n",
        "    fcn_8_crop = Cropping2D(((2,2,),(2,2)), name=\"crop_fcn8_2\")(fcn_8_deconv)\n",
        "    fcn_8_softmax=Softmax(name=\"softmax_fcn8\")(fcn_8_crop)\n",
        "    return Model(input_layer, fcn_8_softmax)\n",
        "  else:\n",
        "    return Model(input_layer, fcn_8_block)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt3PMJLOiryR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfQXk6LViupR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AdaptedMeanIoU(tf.keras.metrics.Metric):\n",
        "  def __init__(self, name='mean_iou', n_classes=3,**kwargs):\n",
        "    super(AdaptedMeanIoU, self).__init__(name=name, **kwargs)\n",
        "    self.mean_iou=MeanIoU(n_classes)\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred_converted=tf.argmax(y_pred,axis=-1)\n",
        "    self.mean_iou.update_state(y_true,y_pred_converted)\n",
        "\n",
        "  def result(self):\n",
        "    return self.mean_iou.result()\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.mean_iou.reset_states()\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=1, min_lr=1**-10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQjXg1oziwMO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "e3927d97-6557-4894-f9f2-e2e071a90881"
      },
      "source": [
        "model=create_fcn8()\n",
        "#model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf1x8ubXpfJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=SGD(learning_rate=0.1, momentum=0.9, clipnorm=1), \n",
        "              loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[AdaptedMeanIoU(n_classes=22)])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBoed5D-i3g1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1cc71847-1401-465d-ea6b-4e3fa66f6666"
      },
      "source": [
        "file_list = os.listdir(dataset_path + '/SegmentationClassConvertedMin')\n",
        "def generate_data(directory, batch_size):\n",
        "    \"\"\"Replaces Keras' native ImageDataGenerator.\"\"\"\n",
        "    i = 0\n",
        "    file_list = os.listdir(directory)\n",
        "    while True:\n",
        "        image_batch_x = []\n",
        "        image_batch_y = []\n",
        "        for b in range(batch_size):\n",
        "            if i == len(file_list):\n",
        "                i = 0\n",
        "                random.shuffle(file_list)\n",
        "            sample = file_list[i]\n",
        "            i += 1\n",
        "            name = sample.split(\".\")[0]\n",
        "            img_x = cv2.imread(dataset_path + '/JPEGImagesMax/' + name + '.jpg',1)\n",
        "            image_batch_x.append(normalize(img_x).numpy())\n",
        "    \n",
        "            with open(dataset_path + '/SegmentationClassConvertedMin/' + name + \".npy\", 'rb') as f:\n",
        "                image_batch_y.append(np.load(f))\n",
        "                \n",
        "        yield [np.array(image_batch_x), np.array(image_batch_y)]\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, batch_size=32, dim=(500,500), n_channels=3,\n",
        "                 n_classes=22, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        print(\"len\")\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        print(\"getitem\")\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self, logs=None):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "        print(\"mischio\")\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, self.dim[0], self.dim[1], self.n_channels))\n",
        "        y = np.empty((self.batch_size, self.dim[0], self.dim[1]), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            #X[i,] = np.load('data/' + ID + '.npy')\n",
        "            img_x = normalize(cv2.imread(dataset_path + '/JPEGImagesMax/' + name + '.jpg',1))\n",
        "            X[i,] = img_x\n",
        "\n",
        "            # Store class\n",
        "            with open(dataset_path + '/SegmentationClassConvertedMin/' + name + \".npy\", 'rb') as f:\n",
        "                img_y = np.load(f)\n",
        "            y[i] = img_y\n",
        "        \n",
        "        print(\"datageneration\")\n",
        "        return X, y\n",
        "        \n",
        "#y_train.shape[0]//BATCH_SIZE//VAL_SUBSPLITS\n",
        "gen = DataGenerator2(list_IDs=os.listdir('/content/SegmentationClassConvertedMin'), \n",
        "                     dataset_path='/content',\n",
        "                     to_fit=True,\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     dim=(500,500),\n",
        "                     n_channels=3,\n",
        "                     n_classes=22,\n",
        "                     shuffle=True)\n",
        "#generate_data(dataset_path + '/SegmentationClassConvertedMin', BATCH_SIZE)\n",
        "model_history = model.fit(gen, \n",
        "                          epochs=EPOCHS,\n",
        "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                          callbacks=[DisplayCallback(),reduce_lr])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-281c46e05ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m                           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                           callbacks=[DisplayCallback(),reduce_lr])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2826\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m           \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3209\u001b[0m           and call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_define_function_with_shape_relaxation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3141\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3142\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3143\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:757 train_step\n        self.trainable_variables)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2745 _minimize\n        experimental_aggregate_gradients=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:519 apply_gradients\n        self._create_all_weights(var_list)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:704 _create_all_weights\n        self._create_slots(var_list)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/gradient_descent.py:125 _create_slots\n        self.add_slot(var, \"momentum\")\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:764 add_slot\n        initial_value=initial_value)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:262 __call__\n        return cls._variable_v2_call(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:256 _variable_v2_call\n        shape=shape)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2857 creator\n        return next_creator(**kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter\n        return captured_getter(captured_previous, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py:702 invalid_creator_scope\n        \"tf.function-decorated function tried to create \"\n\n    ValueError: tf.function-decorated function tried to create variables on non-first call.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvgYE31tAwft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QBvBxPSsBOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPEDz_VFJPfP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e_TVhLULNLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBVMHaxjRWDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "\n",
        "class DataGenerator2(Sequence):\n",
        "    \"\"\"Generates data for Keras\n",
        "    Sequence based data generator. Suitable for building data generator for training and prediction.\n",
        "    \"\"\"\n",
        "    def __init__(self, list_IDs, dataset_path,\n",
        "                 to_fit=True, batch_size=32, dim=(256, 256),\n",
        "                 n_channels=1, n_classes=10, shuffle=True):\n",
        "        self.list_IDs = list_IDs\n",
        "        self.dataset_path=dataset_path\n",
        "        self.to_fit = to_fit\n",
        "        self.batch_size = batch_size\n",
        "        self.dim = dim\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Denotes the number of batches per epoch\n",
        "        :return: number of batches per epoch\n",
        "        \"\"\"\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Generate one batch of data\n",
        "        :param index: index of the batch\n",
        "        :return: X and y when fitting. X only when predicting\n",
        "        \"\"\"\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X = self._generate_X(list_IDs_temp)\n",
        "\n",
        "        if self.to_fit:\n",
        "            y = self._generate_y(list_IDs_temp)\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\n",
        "        \"\"\"\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def _generate_X(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size images\n",
        "        :param list_IDs_temp: list of label ids to load\n",
        "        :return: batch of images\n",
        "        \"\"\"\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            img_x = normalize(cv2.imread(dataset_path + '/JPEGImagesMax/' + name + '.jpg',1))\n",
        "            X[i,] = img_x\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _generate_y(self, list_IDs_temp):\n",
        "        \"\"\"Generates data containing batch_size masks\n",
        "        :param list_IDs_temp: list of label ids to load\n",
        "        :return: batch if masks\n",
        "        \"\"\"\n",
        "        y = np.empty((self.batch_size, *self.dim), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            with open(dataset_path + '/SegmentationClassConvertedMin/' + name + \".npy\", 'rb') as f:\n",
        "                img_y = np.load(f)\n",
        "            y[i] = img_y\n",
        "\n",
        "        return y\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJnAqE4BV4oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}